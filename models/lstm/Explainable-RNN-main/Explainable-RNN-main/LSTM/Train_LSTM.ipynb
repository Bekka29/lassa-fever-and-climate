{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35cc401-151e-4bcd-b7a0-b401fa6b2357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "from comet_ml.utils import safe_filename\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable, variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from scipy.special import inv_boxcox\n",
    "import os \n",
    "import random\n",
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#Set seeds for all processes\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29306514-6e04-4e85-9841-bd6204ff4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "hyper_params={\n",
    "    \"alpha\" : 0.008,\n",
    "    \"dropout\":0.05,\n",
    "    \"hidden_size\" : 128,\n",
    "    \"n_epochs\" : 30, \n",
    "    \"num_layers\" : 2,\n",
    "    \"input_size\" : 6,\n",
    "    \"sequence_length\" : 92,\n",
    "    \"batch_size\" : 3000,\n",
    "}\n",
    "\n",
    "#Import data\n",
    "Raw_data = pd.read_csv(\"../Data/Raw_data.csv\",index_col=0)\n",
    "Training_df = pd.read_csv(\"../Data/Train.csv\",index_col=0)\n",
    "Val_df = pd.read_csv(\"../Data/Val.csv\",index_col=0)\n",
    "Test_df = pd.read_csv(\"../Data/Test.csv\",index_col=0)\n",
    "\n",
    "#Import data required to undo normalization\n",
    "#import the list of min_maxs for normalization\n",
    "min_max= []\n",
    "with open('../Data/Min_max.txt', 'r') as file:\n",
    "    contents = file.readlines()\n",
    "    for i in contents:\n",
    "        i.strip(\"''\")\n",
    "        line = i[1:-3].split(\",\")\n",
    "        min_max.append(list(line))\n",
    "for i in range(len(min_max)):\n",
    "    for j in range(len(min_max[i])):\n",
    "        if j >= 1:\n",
    "            min_max[i][j] = float(min_max[i][j])\n",
    "        else:\n",
    "            min_max[i][j] = min_max[i][j].strip(\"'\")\n",
    "            \n",
    "#import the list of lambdas\n",
    "lambda_list = []\n",
    "with open('../Data/Lambda.txt', 'r') as file:\n",
    "    contents = file.readlines()\n",
    "    for i in contents:\n",
    "        i.strip(\"''\")\n",
    "        line = i[1:-3].split(\",\")\n",
    "        lambda_list.append(list(line))\n",
    "for i in range(len(lambda_list)):\n",
    "    for j in range(len(lambda_list[i])):\n",
    "        if j >= 1:\n",
    "            lambda_list[i][j] = float(lambda_list[i][j])\n",
    "        else:\n",
    "            lambda_list[i][j] = lambda_list[i][j].strip(\"'\")\n",
    "\n",
    "#Import Trend Data\n",
    "Trend = pd.read_csv(\"../Data/Trend.csv\",index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53711959-510d-49b1-8d0c-0df2c0c383ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Validation and Test dataset\n",
    "#Add the last 92 entries from Training to Validation. \n",
    "Val_df2 = Training_df.iloc[-99:,:]._append(Val_df)\n",
    "\n",
    "#Add the last 92 entries from Validation to Test. \n",
    "Test_df2 = Val_df.iloc[-99:,:]._append(Test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e64382d1-1579-41c5-849c-3c68671e5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential_Data(Dataset):\n",
    "    def __init__(self, data, window):\n",
    "        self.data = torch.Tensor(data.values)\n",
    "        self.window = window\n",
    "        self.shape = self.__getshape__()\n",
    "        self.size = self.__getsize__()\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.window]\n",
    "        y = self.data[index+6+self.window,3]\n",
    "        return x, y\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data) -  self.window -7\n",
    "    \n",
    "    def __getshape__(self):\n",
    "        return (self.__len__(), *self.__getitem__(0)[0].shape)\n",
    "    \n",
    "    def __getsize__(self):\n",
    "        return (self.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983d80f3-9969-4f87-9f63-38765c43ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMAPE \n",
    "#Calculate accuracy of using the symmetric mean absolute percentage error\n",
    "def SMAPE(Predicted, Actual):\n",
    "    n= len(Predicted)\n",
    "    numer = abs(Predicted-Actual)\n",
    "    denom = (abs(Actual) + abs(Predicted))*0.5\n",
    "    return (100/n) * (np.sum(numer/denom))\n",
    "\n",
    "def to_raw(Results,Data):\n",
    "    if type(Results) == torch.Tensor:\n",
    "        x = np.array(Results.cpu().detach())\n",
    "    else :\n",
    "        x = np.array(Results.cpu())\n",
    "    Un_normal = (x*(min_max[3][2]-min_max[3][1])) + min_max[3][1]\n",
    "    \n",
    "    if Data==\"Val\":\n",
    "        Seasonal = Un_normal + np.array(Trend.iloc[-400:-200,3])\n",
    "    elif Data == \"Test\": \n",
    "        Seasonal = Un_normal + np.array(Trend.iloc[-200:,3])\n",
    "    else:\n",
    "        Seasonal = Un_normal + np.array(Trend.iloc[97:-401,3])\n",
    "    \n",
    "    Non_box_cox = inv_boxcox(Seasonal,lambda_list[3][1])\n",
    "    return Non_box_cox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f43f2d-51ef-484f-823b-3903481e0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data\n",
    "Training_data = Sequential_Data(Training_df,hyper_params['sequence_length'])\n",
    "Validation_data = Sequential_Data(Val_df2,hyper_params['sequence_length'])\n",
    "Test_data = Sequential_Data(Test_df2,hyper_params['sequence_length'])\n",
    "\n",
    "# Load into a data loader \n",
    "trainloader = DataLoader(dataset=Training_data,batch_size=300,shuffle=False,num_workers=0)\n",
    "valloader = DataLoader(dataset=Validation_data,batch_size=300,shuffle=False,num_workers=0)\n",
    "testloader = DataLoader(dataset=Test_data,batch_size=len(Test_data),shuffle=False,num_workers=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c9669e6-dd53-4549-81f0-4d1dbaa4563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers,drop_p):\n",
    "        super(LSTM,self).__init__()\n",
    "        #stipulate number of layers and hidden layer size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.drop_p = drop_p\n",
    "        \n",
    "        #Layer\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,dropout=drop_p)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        #fully connect layer\n",
    "        self.fc = nn.Linear(in_features=hidden_size,out_features=1)\n",
    "        #input = [batch_size,number_in_seq,num_features]\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #initialise hidden and cell state\n",
    "        h0 = Variable(torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device))\n",
    "        c0 = Variable(torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device))\n",
    "        # h_initial = [number_of_layers,bath_size,hidden_size] -> [1,100,128]\n",
    "        #Proporgate\n",
    "        output, (hn, cn) = self.lstm(x,(h0,c0))\n",
    "        \n",
    "        #retrieve last hidden state:\n",
    "        last_out = output[:,-1,:] #[n,128]\n",
    "        \n",
    "        #add non-linearity\n",
    "        #non_linear = self.sig(last_out)\n",
    "        non_linear = self.relu(last_out)\n",
    "\n",
    "        #collapse the output to a prediction\n",
    "        prediction = self.fc(non_linear)\n",
    "        #reshape the output \n",
    "        prediction = torch.reshape(prediction,(last_out.size()[0],))\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34c03aac-536c-4904-a6f2-f5993813176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size=hyper_params['input_size'],\n",
    "                                    hidden_size=hyper_params['hidden_size'],\n",
    "                                    num_layers=hyper_params['num_layers'],\n",
    "                                    drop_p=hyper_params['dropout']).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(),lr=hyper_params['alpha'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78ea1a11-249d-4570-9774-009e3a8efaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 92, 6]) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "for i, (train_sequence, train_forecast) in enumerate(trainloader):\n",
    "    print(train_sequence.shape, train_forecast.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95533d03-8a9f-4150-a35e-08a37a694a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model \n",
    "#make sure the model parameters are reset before training\n",
    "#with experiment.train():\n",
    "print(\"alpha: \", hyper_params['alpha'], \"dropout: \",hyper_params['dropout'],\"hidden: \",\n",
    "      hyper_params['hidden_size'],'layers: ',hyper_params['num_layers'])\n",
    "model.train()\n",
    "for epoch in range(hyper_params['n_epochs']):\n",
    "    with torch.no_grad():\n",
    "        for j, (val_sequence, val_forecast) in enumerate(valloader): \n",
    "        # Forward pass\n",
    "            val_sequence=Variable(val_sequence).to(device)\n",
    "            val_forecast = Variable(val_forecast).to(device)\n",
    "            val_outputs = model(val_sequence)\n",
    "            Val_loss = criterion(val_outputs,val_forecast)\n",
    "    #experiment.log_metric(\"Validation Loss\",Val_loss,step = epoch)\n",
    "\n",
    "    for i, (train_sequence, train_forecast) in enumerate(trainloader): \n",
    "        train_sequence = Variable(train_sequence).to(device)\n",
    "        train_forecast = Variable(train_forecast).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        opt.zero_grad() #zero gradients\n",
    "        outputs = model(train_sequence) #input sequence into model\n",
    "        loss = criterion(outputs,train_forecast) # calculate MSE \n",
    "        #experiment.log_metric(\"loss\",loss,step = epoch) #log the results \n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "\n",
    "    print('Epoch [%d/%d], Loss: %.4f'\n",
    "        % (epoch + 1, hyper_params['n_epochs'],  loss.item()))\n",
    "\n",
    "trainloader = DataLoader(dataset=Training_data,batch_size=300,shuffle=False,num_workers=0)\n",
    "valloader = DataLoader(dataset=Validation_data,batch_size=300,shuffle=False,num_workers=0)\n",
    "testloader = DataLoader(dataset=Test_data,batch_size=len(Test_data),shuffle=False,num_workers=0)    \n",
    "model.eval()\n",
    "# Test model on data\n",
    "Results_list =[]\n",
    "for i, (val_sequence, val_forecast) in enumerate(valloader): \n",
    "    val_sequence = Variable(val_sequence).to(device)\n",
    "    val_forecast = Variable(val_forecast).to(device)\n",
    "\n",
    "    val_out = model(val_sequence)\n",
    "    Results_list.append(np.array(val_out.cpu().detach()))\n",
    "\n",
    "All_results = np.concatenate(Results_list, axis=0)\n",
    "Predicted = to_raw(Results=val_out, Data=\"Val\")\n",
    "Actual =np.array(Raw_data.iloc[-400:-200,3])\n",
    "Val_SMAPE = SMAPE(Actual=Actual,Predicted=Predicted)\n",
    "#experiment.log_metric(\"Validation Accuracy\",Val_SMAPE)\n",
    "\n",
    "for j, (test_sequence, test_forecast) in enumerate(testloader): \n",
    "    test_sequence = Variable(test_sequence).to(device)\n",
    "    test_forecast = Variable(test_forecast).to(device)\n",
    "\n",
    "    Test_out = model(test_sequence)\n",
    "    Predicted = to_raw(Results=Test_out, Data= \"Test\")\n",
    "    Actual =np.array(Raw_data.iloc[-200:,3])\n",
    "    Test_SMAPE = SMAPE(Actual=Actual,Predicted=Predicted)\n",
    "    #experiment.log_metric(\"Test Accuracy\",Test_SMAPE)\n",
    "print(Test_SMAPE)\n",
    "\n",
    "torch.save(model.state_dict(), \"../Weights/LSTM.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
